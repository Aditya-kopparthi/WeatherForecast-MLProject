# -*- coding: utf-8 -*-
"""MyWeatherNew.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CfuqFrgOYlOsOPGEIFmHOIDqXubJJpiQ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Load the dataset
file_path2 = 'Project1WeatherDataset.csv'
df2 = pd.read_csv(file_path2)

df2.head()

# Create a copy of the original dataframe to preserve data
df2_filled = df2.copy()

# List of numeric columns to fill missing values with mean
numeric_cols = ['Temp_C', 'Dew Point Temp_C', 'Rel Hum_%', 'Wind Speed_km/h', 'Visibility_km', 'Press_kPa']

# Fill missing values for numeric columns with their mean
for col in numeric_cols:
    df2_filled[col] = df2_filled[col].fillna(df2_filled[col].mean())

# Fill missing values for the categorical column 'Weather' with its mode (most frequent value)
df2_filled['Weather'] = df2_filled['Weather'].fillna(df2_filled['Weather'].mode()[0])

# Encode the target variable (Weather) using LabelEncoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df2_filled['Weather'] = le.fit_transform(df2_filled['Weather'])

# Assume df2 has a 'Date/Time' column in string format
df2['Date/Time'] = pd.to_datetime(df2['Date/Time'])

# Extract time features
df2_filled['Hour'] = df2['Date/Time'].dt.hour
df2_filled['DayOfWeek'] = df2['Date/Time'].dt.dayofweek  # Monday=0, Sunday=6
df2_filled['Month'] = df2['Date/Time'].dt.month
df2_filled['DayOfYear'] = df2['Date/Time'].dt.dayofyear

df2_filled.head()

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['Date/Time']
  ys = series['Dew Point Temp_C']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = _df_9.sort_values('Date/Time', ascending=True)
for i, (series_name, series) in enumerate(df_sorted.groupby('Date/Time')):
  _plot_series(series, series_name, i)
  fig.legend(title='Date/Time', bbox_to_anchor=(1, 1), loc='upper left')
sns.despine(fig=fig, ax=ax)
plt.xlabel('Date/Time')
_ = plt.ylabel('Dew Point Temp_C')

from matplotlib import pyplot as plt
_df_2['Rel Hum_%'].plot(kind='hist', bins=20, title='Rel Hum_%')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
_df_0['Temp_C'].plot(kind='hist', bins=20, title='Temp_C')
plt.gca().spines[['top', 'right',]].set_visible(False)

from matplotlib import pyplot as plt
_df_1['Dew Point Temp_C'].plot(kind='hist', bins=20, title='Dew Point Temp_C')
plt.gca().spines[['top', 'right',]].set_visible(False)

# Assuming your DataFrame is named df2
df2_filled = df2_filled.drop(columns=['Date/Time'])

df2_filled.head()

from sklearn.preprocessing import StandardScaler

# Define the features that need to be normalized
features_to_normalize = ['Temp_C', 'Dew Point Temp_C', 'Rel Hum_%',
                         'Wind Speed_km/h', 'Visibility_km', 'Press_kPa',
                         'Hour', 'DayOfWeek', 'Month', 'DayOfYear']

# Initialize the scaler and apply it to the features
scaler = StandardScaler()
df2_filled[features_to_normalize] = scaler.fit_transform(df2_filled[features_to_normalize])

# Check the first few rows after normalization
print(df2_filled.head())

import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix
corr_matrix = df2_filled.corr()
print("Correlation of all features with Weather:")
print(corr_matrix['Weather'])

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix of Normalized Features and Weather")
plt.show()

# Drop the redundant columns: 'Dew Point Temp_C' and 'Month'
df2_filled = df2_filled.drop(columns=['Dew Point Temp_C', 'Month'])

# Display the first few rows to confirm the columns have been removed
print(df2_filled.head())

# Separate the features (all columns except 'Weather') and the target ('Weather')
X = df2_filled.drop(columns=['Weather'])
y = df2_filled['Weather']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.ensemble import RandomForestClassifier

# Create and train a baseline Random Forest model
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Make predictions on the test set
y_pred = rf.predict(X_test)

# Generate and print the classification report
print(classification_report(y_test, y_pred))

# Compute the confusion matrix and visualize it
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Random Forest Baseline")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import numpy as np

# Retrieve feature importances from the model
importances = rf.feature_importances_
feature_names = X.columns

# Plot feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x=importances, y=feature_names)
plt.title("Feature Importance from RandomForest")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Define the parameter distribution for RandomizedSearchCV
param_distributions = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize the baseline Random Forest model
rf = RandomForestClassifier(random_state=42)

# Setup the RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_distributions,
    n_iter=10,          # Number of parameter settings sampled
    cv=3,               # 3-fold cross-validation
    n_jobs=-1,          # Use all available cores
    random_state=42,
    verbose=2
)

# Fit RandomizedSearchCV on the training data
random_search.fit(X_train, y_train)

# Output the best parameters found
print("Best Parameters:", random_search.best_params_)

# Retrieve the best estimator (the tuned model)
tuned_model = random_search.best_estimator_

# Evaluate the tuned model on the test set
y_pred_tuned = tuned_model.predict(X_test)
print("\nTuned Model Classification Report:\n", classification_report(y_test, y_pred_tuned))

# Plot the confusion matrix for the tuned model
cm_tuned = confusion_matrix(y_test, y_pred_tuned)
plt.figure(figsize=(10, 8))
sns.heatmap(cm_tuned, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Tuned RandomForest Model")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

# Assume rf is your RandomForestClassifier and X_train, y_train are your training data
train_sizes, train_scores, validation_scores = learning_curve(
    estimator=rf,
    X=X_train,
    y=y_train,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

# Compute average scores for each training size
train_scores_mean = train_scores.mean(axis=1)
validation_scores_mean = validation_scores.mean(axis=1)

# Plot the learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')
plt.plot(train_sizes, validation_scores_mean, 'o-', color='green', label='Validation score')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.title('Learning Curve')
plt.legend(loc='best')
plt.show()

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split

# Let's assume X and y are your features and target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest with more restrictive hyperparameters
rf_simple = RandomForestClassifier(
    max_depth=10,        # Restrict tree depth
    min_samples_leaf=4,  # Each leaf must have at least 4 samples
    random_state=42
)
rf_simple.fit(X_train, y_train)

# Evaluate
y_pred_simple = rf_simple.predict(X_test)
print("Classification Report with Reduced Complexity:\n", classification_report(y_test, y_pred_simple))

from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [2, 4, 6],
    'max_features': ['auto', 'sqrt'],
    'n_estimators': [100, 200]
}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(
    estimator=rf,
    param_grid=param_grid,
    cv=3,
    n_jobs=-1,
    verbose=2
)
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate the tuned model
y_pred_best = best_model.predict(X_test)
print("Tuned Model Classification Report:\n", classification_report(y_test, y_pred_best))

from sklearn.model_selection import cross_val_score

rf_cv = RandomForestClassifier(random_state=42, max_depth=10, min_samples_leaf=4)
scores = cross_val_score(rf_cv, X, y, cv=5, scoring='accuracy')
print("Cross-validation scores:", scores)
print("Average accuracy:", scores.mean())

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import learning_curve

train_sizes, train_scores, validation_scores = learning_curve(
    estimator=rf_simple,
    X=X_train,
    y=y_train,
    train_sizes=np.linspace(0.1, 1.0, 5),
    cv=3,
    scoring='accuracy',
    n_jobs=-1
)

train_scores_mean = np.mean(train_scores, axis=1)
validation_scores_mean = np.mean(validation_scores, axis=1)

plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores_mean, 'o-', color='blue', label='Training score')
plt.plot(train_sizes, validation_scores_mean, 'o-', color='green', label='Validation score')
plt.title('Learning Curve')

